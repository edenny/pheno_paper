# Plant Phenology Data Processing Procedures

This repository was developed to contain all of the scripts for processing plant phenology data sources as required for the construction of a plant phenology paper, looking at Birch and Common Sunflower datasources coming from  the NPN and PEP725 projects.   However, we have made the processes modular enough so more datasources and more species can be easily ingested using the same tools.  The processes here cover data acquisition, pre-processing, validation, triplification, reasoning, and indexing of data sources.

Note that this repository is focused specifically on data processing scripts and does not store actual processed data.  The size of processed datasets is quite large, with an incoming CSV file of 3Mb, potentially consuming up to 500Mb of disk space millions of triples. Processed data files can be either stored in a subdirectory called "data" off of the root directory, or stored elsewhere, as long as file paths are updated in the bin/build.properties file.  We use .gitignore to ignore the data directory contents.

# Required Software

The following software is required to run the tasks for generating reasoned triples for all incoming data and should work on any Linux distribution:

  * Python and [associated PIP libs](https://github.com/jdeck88/pheno_paper/blob/master/requirements.txt)
  * Java 1.8
  * Bash shell
  * **[Biocode FIMS Configurator](https://github.com/biocodellc/biocode-fims-configurator)** The Biocode FIMS Configurator uses a set of easily customizable CSV files and creates a configuration file for the FIMS processes to use in validating and triplifying source
datasets.
  * **[PPO-FIMS Triplifier](https://github.com/biocodellc/ppo-fims)** This is a JAR file built from the PPO-FIMS sources that calls the FIMS system triplifying engine.  We only need the jar that is output from the ppo-fims shadowJar build task (see repository link above for more information)
  * **[Ontopilot](https://github.com/stuckyb/ontopilot)** Ontopilot is mainly used for building ontologies, but can also be used for pre-reasoning unreasoned triples files, which are generated by the PPO=FIMS Triplifier.
  * **[query_fetcher](https://github.com/biocodellc/query_fetcher)** Run SPARQL queries against Reasoned output sources.

# Step 1: Create FIMS configuration file(s) 

The first step involves building the necessary configuration files so we can validate and triplify source data for each project. 
Documentation for this process is located at the [Biocode FIMS Configurator Repository] (https://github.com/biocodellc/biocode-fims-configurator) and create the appropriate directories locally.

E.g. for npn first time only create the following directories and populate with the correct configuration files.
```
  mkdir npn
  mkdir npn/config
```

Now you you will need to run the configurator, calling the buildConfig.sh script and 
passing in your project name.  Note that you must first adjust local variables in build.properties:
```
  cd bin
  ./buildConfig.sh npn
```

Once the configurator does its work and we have succesfully built a configuration you should push 
the completed config file to github (or wherever it should be accessed on the web)
Note that the above script also runs data driven tests against the newly created configuration file. 
 
# Step 2: Processing (Pre-processing, Splitting, Triplifying, and Reasoning)

The steps of pre-processing, splitting, triplifying, and reasoning can be run at once using the bin/runFiles.sh script, like:
```
  cd bin
  ./runFiles.sh <project>
```
When running the script, you will be prompted for files that have been pre-processed and ready to go in the output_csv directory.

*Pre-processing* is done using python scripts in the bin/preProcessor directory structure and are named bin/preProcessor[project]/process.py  Each of the project processing scripts are slightly different, and have a goal of converting various formats into a standardized CSV file with all relevant data.  Note that the project preProcessing directories contain various helper scripts for working with the data.
*Splitting data* is done using a python script called fileSplitter.py stored in the bin directory and called from the runFiles.sh script (above).  The file splitter splits incoming CSV files into 50,000 records or less.  We have found that running more than 50,000 records at a time will cause the triplifier and reasoner to slow down and crash.  


*Triplifying FIMS* data is done using the [ppo-fims java code-base](https://github.com/biocodellc/ppo-fims) which loads tabular data into a temporary SQLITE database, runs a series of validation rules on the data itself, and, if it passes, calls D2RQ for creating RDF triples from the loaded data.  


*Reasoning* is done using the Ontopilot project to pre-reason data sources.  The current (and temporary! process) is to run this through the ppo_pre_reasoner (called through a shell script).  Note that for this step we need to check out the following repositories:

https://github.com/plantphenoontology/ppo_pre_reasoner/

https://github.com/stuckyb/ontopilot  (for now, use the elk_pipeline branch)


# Step 3: Data Storage / Indexing

*SPARQL Endpoint *

Reasoned data is being loaded into a BlazeGraph SPARQL endpoint at http://data.plantphenology.org/  ..  Email the owner of this repository if you would like access to this host if you want to try out queries.  Note that we do not yet have a stable namespace for loaded SPARQL queries so you should be careful about using the most up to date namespace for loaded data.  One can use the GUI interface for running queries, or instead issue SPARQL by calling the blazegraph REST API.

A very simple SPARQL command to get data back from this endpoint for the pheno_paper3 namespace is here:

```
curl -X POST http://data.plantphenology.org/blazegraph/namespace/pheno_paper3/sparql --data-urlencode 'query=SELECT * { ?s ?p ?o } LIMIT 1' -H 'Accept:text/csv'
```

A more complex curl statement that returns flowering time, day of year, and year for Genus = "Helianthus":

```
curl -X POST http://data.plantphenology.org/blazegraph/namespace/pheno_paper3/sparql -H 'Accept:text/csv' --data-urlencode 'query=prefix dwc: <http://rs.tdwg.org/dwc/terms/> prefix obo: <http://purl.obolibrary.org/obo/>  SELECT  ?startDayOfYear ?year ?latitude ?longitude ?wholePlant WHERE {    ?wholePlant dwc:genus "Helianthus"^^<http://www.w3.org/2001/XMLSchema#string> . optional{?wholePlant dwc:specificEpithet "annuus"^^<http://www.w3.org/2001/XMLSchema#string>} . ?wholePlant obo:RO_0000086 ?plantStructurePresence . ?plantStructurePresence rdf:type obo:PPO_0003010 . ?plantStructurePresence obo:PPO_0001007 ?measurementDatum . ?measurementDatum obo:OBI_0000312 ?phenologyObservingProcess . ?phenologyObservingProcess rdf:type obo:PPO_0002000 . ?phenologyObservingProcess dwc:startDayOfYear ?startDayOfYear . ?phenologyObservingProcess dwc:year ?year . ?phenologyObservingProcess dwc:decimalLatitude ?latitude . ?phenologyObservingProcess dwc:decimalLongitude ?longitude . } ORDER BY ?startDayOfYear ?year'
```

Here is the SPARQL in the above statement:

```
prefix dwc: <http://rs.tdwg.org/dwc/terms/> 
prefix obo: <http://purl.obolibrary.org/obo/>  
SELECT  ?startDayOfYear ?year ?latitude ?longitude ?wholePlant 
WHERE {    
	?wholePlant dwc:genus "Helianthus"^^<http://www.w3.org/2001/XMLSchema#string> . 
	optional{ ?wholePlant dwc:specificEpithet "annuus"^^<http://www.w3.org/2001/XMLSchema#string> } . 

	# wholePlant 'hasQuality' some plantStructurePresence
	?wholePlant obo:RO_0000086 ?plantStructurePresence . 

	# search for flower heads present
	?plantStructurePresence rdf:type obo:PPO_0003010 . 

	# plantStructurePresence 'has quality measurement' some measurementDatum
	?plantStructurePresence obo:PPO_0001007 ?measurementDatum . 
	
	# measurementDatum 'is_specified_output_of' some phenologyObservingProcess
	?measurementDatum obo:OBI_0000312 ?phenologyObservingProcess . 

	# set the type for phenologyObservingProcess and return properties
	?phenologyObservingProcess rdf:type obo:PPO_0002000 . 
	?phenologyObservingProcess dwc:startDayOfYear ?startDayOfYear . 
	?phenologyObservingProcess dwc:year ?year . 
	?phenologyObservingProcess dwc:decimalLatitude ?latitude . 
	?phenologyObservingProcess dwc:decimalLongitude ?longitude . 
} ORDER BY ?startDayOfYear ?year'
```
*ElasticSearch* 

Data loading into ElasticSearch still under development... nothing to report here for now.

# NOTES

**Project Configuration Directories** Configuration files for each projects are stored off of the root path and are named for each project (e.g. npn, pep725).  

**Data Directory** A data directory is assumed (with a configurable location, so it can be located on partitions with more drive space), with subdirectories for each project.  The required subdirectories are: output_csv, output_csv_split, output_unreasoned_n3, output_reasoned_owl

**Data Collection** A list of some samples of plant phenology data sources are stored under the [Jan, 2016 plant phenology "workshop" data repository] (https://github.com/plantPhenoOntology/pheno_data).


